{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ContextualVersionConflict",
     "evalue": "(scipy 1.4.1 (/Users/ujwalkumar/anaconda3/lib/python3.7/site-packages), Requirement.parse('scipy<1.3.0,>=0.7.0'), {'tkp'})",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mContextualVersionConflict\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-513f1993c01d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtkp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtkp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVarmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtkp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunningcatalog\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtkp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunningcatalogFlux\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtkp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNewsource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tkp-4.0-py3.7.egg/tkp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tkp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"dev\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \"\"\"\n\u001b[0;32m--> 900\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0;31m# Oops, the \"best\" so far conflicts with a dependency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m                 \u001b[0mdependent_req\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mVersionConflict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependent_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mContextualVersionConflict\u001b[0m: (scipy 1.4.1 (/Users/ujwalkumar/anaconda3/lib/python3.7/site-packages), Requirement.parse('scipy<1.3.0,>=0.7.0'), {'tkp'})"
     ]
    }
   ],
   "source": [
    "import tkp.db\n",
    "from tkp.db.model import Varmetric\n",
    "from tkp.db.model import Runningcatalog\n",
    "from tkp.db.model import RunningcatalogFlux\n",
    "from tkp.db.model import Newsource\n",
    "from tkp.db.model import Image\n",
    "from tkp.db.model import Extractedsource\n",
    "from sqlalchemy import *\n",
    "from sqlalchemy.orm import relationship\n",
    "\n",
    "                ### Need to download tkp\n",
    "            \n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import norm\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from dblogin import * # This file contains all the variables required to connect to the database\n",
    "from database_tools import dbtools\n",
    "from tools import tools       ###Use weblib instead\n",
    "from plotting import plot_varib_params as pltvp     ###Can't find this package\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "                ###dblogin, database_tools, tools, plotting are packages I couldn't download\n",
    "\n",
    "from machine_learning import generic_tools\n",
    "from machine_learning import MLtests\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "query_loglevel = logging.WARNING  # Set to INFO to see queries, otherwise WARNING\n",
    "sys.path.append('../')\n",
    "pylab.rcParams['legend.loc'] = 'best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-25bb75aa89f2>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-25bb75aa89f2>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    port =\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "engine = 'postgresql'\n",
    "host = 'http://banana.transientskp.org/r3'\n",
    "port = ''\n",
    "user = 'lofartkp'\n",
    "password = 'grs1915'\n",
    "\n",
    "def access(engine,host,port,user,password,database):\n",
    "    # Access the database using sqlalchemy\n",
    "    db = tkp.db.Database(engine=engine, host=host, port=port,\n",
    "                     user=user, password=password, database=database)\n",
    "    db.connect()\n",
    "    session = db.Session()\n",
    "    print ('connected!')\n",
    "    return session\n",
    "\n",
    "def GetVarParams(session,dataset_id):\n",
    "    # Returns all the variability parameters for sources in a given dataset\n",
    "    VarParams = session.query(Varmetric,Runningcatalog).select_from(join(Varmetric,Runningcatalog)).filter(Runningcatalog.dataset_id == dataset_id).all()\n",
    "    return VarParams\n",
    "\n",
    "def GetTransDataForML(session,dataset_id):\n",
    "    transients = (session.query(Newsource,Runningcatalog)\n",
    "                      .select_from(join(Newsource,Runningcatalog))\n",
    "                      .filter(Runningcatalog.dataset_id == dataset_id)\n",
    "                     .all())\n",
    "    return transients\n",
    "\n",
    "def GetRuncatDataForML(session,dataset_id):\n",
    "    transients = (session.query(Runningcatalog)\n",
    "                      .join(Varmetric)\n",
    "                      .join(RunningcatalogFlux)\n",
    "#                      .select_from(join(Runningcatalog))\n",
    "                     .filter(Runningcatalog.dataset_id == dataset_id)\n",
    "                     .all())\n",
    "    return transients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbtools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9fedf7d4c63d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Connect to the database and run the queries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtransients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetTransDataForML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dbtools' is not defined"
     ]
    }
   ],
   "source": [
    "### CONNECTING TO DATABASE\n",
    "\n",
    "# The input database, dataset and thresholds\n",
    "dataset_id = 9\n",
    "database = 'AR_R4'\n",
    "websiteURL = 'http://banana.transientskp.org/r4/vlo_'+database+'/runningcatalog/'\n",
    "\n",
    "path='ml_csv_files/'\n",
    "stableData = path+'stable_sources.csv'\n",
    " \n",
    "# Connect to the database and run the queries\n",
    "session = dbtools.access(engine,host,port,user,password,database)\n",
    "\n",
    "transients = dbtools.GetTransDataForML(session,dataset_id)\n",
    "transrcs = [[transients[i].Newsource.trigger_xtrsrc.image.detection_thresh,\n",
    "                 transients[i].Newsource.trigger_xtrsrc.f_int,\n",
    "                 transients[i].Newsource.newsource_type,\n",
    "                 transients[i].Newsource.trigger_xtrsrc.image.rms_max,\n",
    "                 transients[i].Newsource.trigger_xtrsrc.image.rms_min,\n",
    "                 transients[i].Runningcatalog.id,\n",
    "                 ] for i in range(len(transients))]\n",
    "\n",
    "\n",
    "Runcats = dbtools.GetRuncatDataForML(session,dataset_id)\n",
    "runcats = [[Runcats[i].xtrsrc.image.band.freq_central,\n",
    "                Runcats[i].dataset.id,\n",
    "                Runcats[i].varmetric.eta_int,\n",
    "                Runcats[i].xtrsrc.extract_type,\n",
    "                Runcats[i].datapoints,\n",
    "                Runcats[i].xtrsrc.f_int,\n",
    "                Runcats[i].xtrsrc.f_int_err,\n",
    "                Runcats[i].xtrsrc.image.freq_eff,\n",
    "                Runcats[i].id,\n",
    "                Runcats[i].xtrsrc.image.tau_time,\n",
    "                Runcats[i].xtrsrc.image.taustart_ts,\n",
    "                Runcats[i].varmetric.v_int,\n",
    "                Runcats[i].wm_decl,\n",
    "                Runcats[i].wm_ra,\n",
    "                Runcats[i].xtrsrc.id\n",
    "             ] for i in range(len(Runcats))]\n",
    "\n",
    "\n",
    "frequencies, srcs = tools.read_src_lc(runcats)\n",
    "\n",
    "print (runcats[-1])\n",
    "print (frequencies)\n",
    "\n",
    "trans_data = tools.collate_trans_data(srcs,frequencies,transrcs)\n",
    "\n",
    "\n",
    "print (trans_data[-1])\n",
    "\n",
    "output3 = open('ds'+str(dataset_id)+'_trans_data.txt','w')\n",
    "output3.write('#Runcat, eta, V, flux, fluxrat, freq, dpts, RA, Dec, ttype, maxRmsSigma, minRmsSigma, detectionThreshold  \\n')\n",
    "for x in range(len(trans_data)):\n",
    "    string='%s' % ','.join(str(val) for val in trans_data[x])\n",
    "    output3.write(string+'\\n')\n",
    "output3.close()\n",
    "print ('Data extracted and saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defined functions\n",
    "\n",
    "def trial_data(args):\n",
    "    # Find the precision and recall for a given pair of thresholds\n",
    "    data,sigma1,sigma2 = args\n",
    "\n",
    "    # Sort data into transient and non-transient\n",
    "    xvals = [float(x[0]) for x in data if float(x[-1]) != 0.]\n",
    "    yvals = [float(x[1]) for x in data if float(x[-1]) != 0.]\n",
    "    xstable = [float(x[0]) for x in data if float(x[-1]) == 0.]\n",
    "    ystable = [float(x[1]) for x in data if float(x[-1]) == 0.]\n",
    "\n",
    "    # Find the thresholds for a given sigma, by fitting data with a Gaussian model\n",
    "    sigcutx,paramx,range_x = generic_tools.get_sigcut([float(x[0]) for x in data if float(x[-1]) == 0.],sigma1)\n",
    "    sigcuty,paramy,range_y = generic_tools.get_sigcut([float(x[1]) for x in data if float(x[-1]) == 0.],sigma2)\n",
    "\n",
    "    # Count up the different numbers of tn, tp, fp, fn\n",
    "    fp=len([z for z in range(len(xstable)) if (xstable[z]>sigcutx and ystable[z]>sigcuty)]) # False Positive\n",
    "    tn=len([z for z in range(len(xstable)) if (xstable[z]<sigcutx or ystable[z]<sigcuty)]) # True Negative\n",
    "    tp=len([z for z in range(len(xvals)) if (xvals[z]>sigcutx and yvals[z]>sigcuty)]) # True Positive\n",
    "    fn=len([z for z in range(len(xvals)) if (xvals[z]<sigcutx or yvals[z]<sigcuty)]) # False Negative\n",
    "    \n",
    "    # Use these values to calculate the precision and recall values\n",
    "    precision, recall = generic_tools.precision_and_recall(tp,fp,fn)\n",
    "    return [sigma1, sigma2, precision, recall]\n",
    "\n",
    "def multiple_trials(data,filename):\n",
    "    # Find the precision and recall for all combinations of the sigma thresholds\n",
    "    sigmas=np.arange(0.,3.5,(3.5/500.))\n",
    "    pool = Pool(processes=20)              # start 20 worker processes\n",
    "    inputs = range(2)\n",
    "    \n",
    "    # Loop through all the trial sigmas and on multiple workers, then append to a file\n",
    "    for sigma1 in sigmas:\n",
    "        sigma_data = pool.map(trial_data, [(data,sigma1,sigma2) for sigma2 in sigmas])\n",
    "        with open(filename,'a') as f_handle:\n",
    "            np.savetxt(f_handle,sigma_data)\n",
    "        f_handle.close()\n",
    "    pool.close() # Close the worker processes once training is complete\n",
    "    return\n",
    "\n",
    "def tests(args):\n",
    "    # Test multiple input precision and recall values to check out if we are meeting and exceeding the input parameters\n",
    "    xi,yi,zi1,zi2, data, xvals, yvals, xstable, ystable, precis, recall = args\n",
    "\n",
    "    # Find the combination of x and y which is closest to the two thresholds\n",
    "    combinations=[[xi[a][b],yi[a][b],zi1[a][b],zi2[a][b]] for a in range(len(zi1)) for \n",
    "                  b in range(len(zi1[0])) if zi1[a][b]>=precis]\n",
    "    ID=np.array([((a[2]-precis)**2. + (a[3]-recall)**2.) for a in combinations]).argmin()\n",
    "    above_thresh_sigma=combinations[ID]\n",
    "    \n",
    "    # Find the thresholds for these sigmas, by fitting the observed data with a Gaussian model\n",
    "    sigcutx,paramx,range_x = generic_tools.get_sigcut([float(x[0]) for x in data],above_thresh_sigma[0])\n",
    "    sigcuty,paramy,range_y = generic_tools.get_sigcut([float(x[1]) for x in data],above_thresh_sigma[1])\n",
    "\n",
    "    # Count up the different numbers of tp, fp, fn\n",
    "    fp=len([z for z in range(len(xstable)) if (xstable[z]>sigcutx and ystable[z]>sigcuty)]) # False Positive\n",
    "    tp=len([z for z in range(len(xvals)) if (xvals[z]>sigcutx and yvals[z]>sigcuty)]) # True Positive\n",
    "    fn=len([z for z in range(len(xvals)) if (xvals[z]<sigcutx or yvals[z]<sigcuty)]) # False Negative\n",
    "    \n",
    "    # Use these values to calculate the precision and recall values obtained with the trained threshold.\n",
    "    # If the test is successful, the outputs should meet or exceed the input parameters.\n",
    "    results1, results2 = generic_tools.precision_and_recall(tp,fp,fn)\n",
    "\n",
    "    return [precis, recall, results1, results2]\n",
    "\n",
    "def check_method_works(xi,yi,zi1,zi2, data,above_thresh_sigma,path):\n",
    "    # Multiple trials using input precisions and recalls between 0 and 1 are conducted to confirm that this method provides the input precision and recall\n",
    "    trials = np.arange(0.0,1.,1./500.)\n",
    "    trials2 = np.arange(0.0,1.,1./500.)\n",
    "    Z1=[]\n",
    "    Z2=[]\n",
    "    X=[]\n",
    "    Y=[]\n",
    "\n",
    "    print \"Producing anomaly detection test data\"\n",
    "    if not os.path.exists(path+'anomaly_test_data.txt'): # Only run the test if the output data are not already available\n",
    "        open(path+'anomaly_test_data.txt','w').close()\n",
    "        pool = Pool(processes=20)              # start 20 worker processes\n",
    "        inputs = range(2)\n",
    "\n",
    "        # Sort data into variable and non-variable\n",
    "        xvals = [float(x[0]) for x in data if float(x[-1]) != 0.]\n",
    "        yvals = [float(x[1]) for x in data if float(x[-1]) != 0.]\n",
    "        xstable = [float(x[0]) for x in data if float(x[-1]) == 0.]\n",
    "        ystable = [float(x[1]) for x in data if float(x[-1]) == 0.]\n",
    "\n",
    "        # Run through each of the precision and recall trials and calculate the output precision and recalls\n",
    "        for precis in trials:\n",
    "            test_data = pool.map(tests, [(xi,yi,zi1,zi2, data, xvals, yvals, xstable, ystable, precis, recall) for recall in trials2])\n",
    "            with open(path+'anomaly_test_data.txt','a') as f_handle: # append data to a file\n",
    "                np.savetxt(f_handle,test_data)\n",
    "            f_handle.close()\n",
    "        pool.close() # close all the workers\n",
    "\n",
    "    # sort the data for plotting\n",
    "    test_data = np.genfromtxt(path+'anomaly_test_data.txt')\n",
    "    X = [x[0] for x in test_data] # input precision\n",
    "    Y = [x[1] for x in test_data] # input recall\n",
    "    Z1 = [(x[2]-x[0]) for x in test_data] # output precision\n",
    "    Z2 = [(x[3]-x[1]) for x in test_data] # output precision\n",
    "    \n",
    "    print(\"Anomaly Detection test data collated, now gridding\")\n",
    "    xi,yi = np.mgrid[0:1:1000j, 0:1:1000j]\n",
    "    zi1 = griddata((X, Y), Z1, (xi, yi), method='cubic')\n",
    "    zi2 = griddata((X, Y), Z2, (xi, yi), method='cubic')\n",
    "    # Plot results\n",
    "    # Settings\n",
    "    nullfmt   = NullFormatter()         # no labels\n",
    "    fig = plt.figure(1,figsize=(5,10))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    cax = fig.add_axes([0.1, 0.95, 0.8, 0.03])\n",
    "    fig.subplots_adjust(hspace = .001, wspace = 0.001)\n",
    "    levels=np.arange(-0.2,0.25,0.05)\n",
    "    ax2.set_xlabel(r'Input Precision', fontsize=24)\n",
    "    ax1.set_ylabel(r'Input Recall', fontsize=24)\n",
    "    ax2.set_ylabel(r'Input Recall', fontsize=24)\n",
    "    fontP = FontProperties()\n",
    "    fontP.set_size('x-small')\n",
    "    ax1.set_xlim([0.0,1.0])\n",
    "    ax2.set_xlim([0.0,1.0])\n",
    "    ax1.set_ylim([0.0,0.999])\n",
    "    ax2.set_ylim([0.0,0.999])\n",
    "    ax1.xaxis.set_major_formatter(nullfmt)\n",
    "    ax2.set_xlim( ax1.get_xlim() )\n",
    "    ax1.text(2.5, 3.5, 'Precision', bbox=dict(facecolor='white'), fontsize=24)\n",
    "    ax2.text(2.5, 3.5, 'Recall', bbox=dict(facecolor='white'), fontsize=24)\n",
    "    CS1 = ax1.contourf(xi,yi,zi1,levels, cmap=plt.get_cmap('RdBu'),alpha=1,extend='both')\n",
    "    CS2 = ax2.contourf(xi,yi,zi2,levels, cmap=plt.get_cmap('RdBu'),alpha=1,extend='both')\n",
    "    fig.colorbar(CS2, cax, orientation='horizontal')\n",
    "    ax1.axvline(x=above_thresh_sigma[0], linewidth=2, color='k', linestyle='--')\n",
    "    ax2.axvline(x=above_thresh_sigma[0], linewidth=2, color='k', linestyle='--')\n",
    "    ax1.axhline(y=above_thresh_sigma[1], linewidth=2, color='k', linestyle='--')\n",
    "    ax2.axhline(y=above_thresh_sigma[1], linewidth=2, color='k', linestyle='--')\n",
    "    plt.savefig(path+'sim_check_precisions_and_recalls.png')\n",
    "    plt.close()\n",
    "    return\n",
    "\n",
    "def find_best_sigmas(precis_thresh,recall_thresh,data,tests, data2,plots,path):\n",
    "    # Gridding the data for precision and recall\n",
    "    X=[float(x[0]) for x in data]\n",
    "    Y=[float(x[1]) for x in data]\n",
    "    Z1=[float(x[2]) for x in data]\n",
    "    Z2=[float(x[3]) for x in data]\n",
    "    xi,yi = np.mgrid[0:3.5:1000j, 0:3.5:1000j]\n",
    "    zi1 = griddata((X, Y), Z1, (xi, yi), method='cubic', fill_value=1.)\n",
    "    zi2 = griddata((X, Y), Z2, (xi, yi), method='cubic', fill_value=0.)\n",
    "\n",
    "    # Find the combination of x and y which is closest to the two thresholds\n",
    "    combinations=[[xi[a][b],yi[a][b],zi1[a][b],zi2[a][b]] for a in range(len(zi1)) for b in range(len(zi1[0])) if zi1[a][b]>=precis_thresh]\n",
    "    ID=np.array([((a[2]-precis_thresh)**2. + (a[3]-recall_thresh)**2.) for a in combinations]).argmin()\n",
    "    above_thresh_sigma=combinations[ID]\n",
    "\n",
    "    if plots:\n",
    "        # Plot results\n",
    "        # Settings\n",
    "        nullfmt   = NullFormatter()         # no labels\n",
    "        fig = plt.figure(1,figsize=(5,10))\n",
    "        ax1 = fig.add_subplot(211)\n",
    "        ax2 = fig.add_subplot(212)\n",
    "        cax = fig.add_axes([0.1, 0.95, 0.8, 0.03])\n",
    "        fig.subplots_adjust(hspace = .001, wspace = 0.001)\n",
    "        levels=np.arange(0.0,1.1,0.1)\n",
    "        levels2=[0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "        ax2.set_xlabel(r'$\\sigma$ threshold ($\\eta_{\\nu}$)', fontsize=18)\n",
    "        ax1.set_ylabel(r'$\\sigma$ threshold ($V_{\\nu}$)', fontsize=18)\n",
    "        ax2.set_ylabel(r'$\\sigma$ threshold ($V_{\\nu}$)', fontsize=18)\n",
    "        fontP = FontProperties()\n",
    "        fontP.set_size('x-small')\n",
    "        ax1.set_xlim([0.0,3.5])\n",
    "        ax2.set_xlim([0.0,3.5])\n",
    "        ax1.set_ylim([0.0,3.49])\n",
    "        ax2.set_ylim([0.0,3.49])\n",
    "        ax1.xaxis.set_major_formatter(nullfmt)\n",
    "        ax2.set_xlim( ax1.get_xlim() )\n",
    "        ax1.text(2.5, 3.0, 'Precision', bbox=dict(facecolor='white'), fontsize=18)\n",
    "        ax2.text(2.5, 3.0, 'Recall', bbox=dict(facecolor='white'), fontsize=18)\n",
    "        CS1 = ax1.contourf(xi,yi,zi1,levels, cmap=plt.get_cmap('Blues'),alpha=1,extend='both')\n",
    "        CS2 = ax2.contourf(xi,yi,zi2,levels, cmap=plt.get_cmap('Blues'),alpha=1,extend='both')\n",
    "        fig.colorbar(CS2, cax, orientation='horizontal')\n",
    "        ax1.axvline(x=above_thresh_sigma[0], linewidth=2, color='k', linestyle='--')\n",
    "        ax2.axvline(x=above_thresh_sigma[0], linewidth=2, color='k', linestyle='--')\n",
    "        ax1.axhline(y=above_thresh_sigma[1], linewidth=2, color='k', linestyle='--')\n",
    "        ax2.axhline(y=above_thresh_sigma[1], linewidth=2, color='k', linestyle='--')\n",
    "        plt.savefig(path+'sim_precisions_and_recalls.png')\n",
    "        plt.close()\n",
    "\n",
    "    print(\"Best sigmas found:\"+str(above_thresh_sigma[0])+', '+str(above_thresh_sigma[1]))\n",
    "    return above_thresh_sigma[0],above_thresh_sigma[1]\n",
    "\n",
    "def learning_curve(stable,variable,train, valid, precis_thresh, recall_thresh, rangeNums,path):\n",
    "    validErr=[]\n",
    "    valid_list=np.unique([x[0] for x in valid])\n",
    "    output=open(path+'sigma_train.txt','w')\n",
    "    trainErr=[]\n",
    "    for num in rangeNums:\n",
    "        if num>0:\n",
    "            filename = open(path+\"temp_sigma_data.txt\", \"w\")\n",
    "            filename.write('')\n",
    "            filename.close()\n",
    "            trainTMP=train[:num,:]\n",
    "            train_list=np.unique([int(x[0]) for x in trainTMP])\n",
    "            multiple_trials([[np.log10(float(x[1])), np.log10(float(x[2])), float(x[-1])] for x in trainTMP if float(x[1]) > 0 if float(x[2]) > 0],path+\"temp_sigma_data.txt\")\n",
    "            data2=np.genfromtxt(path+'temp_sigma_data.txt', delimiter=' ')\n",
    "            data=[[np.log10(float(trainTMP[n][1])),np.log10(float(trainTMP[n][2])),trainTMP[n][5],float(trainTMP[n][-1])] for n in range(num) if float(trainTMP[n][1]) > 0 if float(trainTMP[n][2]) > 0]\n",
    "            best_sigma1, best_sigma2 = find_best_sigmas(precis_thresh,recall_thresh,data2,False,data,False,path)\n",
    "            \n",
    "            # Find the thresholds for a given sigma (in log space)\n",
    "            sigcutx,paramx,range_x = generic_tools.get_sigcut([a[0] for a in data if a[3]==0.],best_sigma1)\n",
    "            sigcuty,paramy,range_y = generic_tools.get_sigcut([a[1] for a in data if a[3]==0.],best_sigma2)\n",
    "\n",
    "            # Calculate the training error\n",
    "            fp=len([[z[0],float(z[1]),float(z[2]),float(z[3]),float(z[4]),'FP'] for z in stable if (float(z[1])>=10.**sigcutx and float(z[2])>=10.**sigcuty) if int(z[0]) in train_list]) # False Positive\n",
    "            fn=len([[z[0],float(z[1]),float(z[2]),float(z[3]),float(z[4]),'FN'] for z in variable if (float(z[1])<10.**sigcutx or float(z[2])<10.**sigcuty) if int(z[0]) in train_list]) # False Negative\n",
    "            trainErr.append(check_error(fp,fn,len(trainTMP)))\n",
    "\n",
    "            # Caluculate the validation error\n",
    "            fp=len([[z[0],float(z[1]),float(z[2]),float(z[3]),float(z[4]),'FP'] for z in stable if (float(z[1])>=10.**sigcutx and float(z[2])>=10.**sigcuty) if int(z[0]) in valid_list]) # False Positive\n",
    "            fn=len([[z[0],float(z[1]),float(z[2]),float(z[3]),float(z[4]),'FN'] for z in variable if (float(z[1])<10.**sigcutx or float(z[2])<10.**sigcuty) if int(z[0]) in valid_list]) # False Negative\n",
    "            validErr.append(check_error(fp,fn,len(valid)))\n",
    "            output.write(str(num)+','+str(trainErr)+','+str(validErr))\n",
    "    output.close()\n",
    "     \n",
    "    return  trainErr, validErr\n",
    "        \n",
    "def random_test(stable,variable,train, valid, precis_thresh, recall_thresh,path):\n",
    "    multiple_trials([[np.log10(float(x[1])), np.log10(float(x[2])), float(x[-1])] for x in train if float(x[1]) > 0 if float(x[2]) > 0],\"temp_sigma_data.txt\")\n",
    "    data2=np.genfromtxt('temp_sigma_data.txt', delimiter=' ')\n",
    "    data=[[np.log10(float(train[n][1])),np.log10(float(train[n][2])),train[n][5],float(train[n][-1])] for n in range(len(train)) if float(train[n][1]) > 0 if float(train[n][2]) > 0]\n",
    "    best_sigma1, best_sigma2 = find_best_sigmas(precis_thresh,recall_thresh,data2,False,data,False,path)\n",
    "\n",
    "    # Find the thresholds for a given sigma (in log space)\n",
    "    sigcutx,paramx,range_x = generic_tools.get_sigcut([a[0] for a in data if a[3]==0.],best_sigma1)\n",
    "    sigcuty,paramy,range_y = generic_tools.get_sigcut([a[1] for a in data if a[3]==0.],best_sigma2)\n",
    "            \n",
    "    trainIDs=[str(int(x[0])) for x in train]\n",
    "    validIDs=[str(int(x[0])) for x in valid]\n",
    "    \n",
    "    # Calculate the training error\n",
    "    fp=len([[z[0],float(z[1]),float(z[2]),float(z[3]),float(z[4]),'FP'] for z in stable if (float(z[1])>=10.**sigcutx and float(z[2])>=10.**sigcuty) if z[0] in trainIDs]) # False Positive\n",
    "    fn=len([[z[0],float(z[1]),float(z[2]),float(z[3]),float(z[4]),'FN'] for z in variable if (float(z[1])<10.**sigcutx or float(z[2])<10.**sigcuty) if z[0] in trainIDs]) # False Negative\n",
    "    trainErr = check_error(fp,fn,len(train))\n",
    "\n",
    "    # Caluculate the validation error\n",
    "    fp=len([[z[0],float(z[1]),float(z[2]),float(z[3]),float(z[4]),'FP'] for z in stable if (float(z[1])>=10.**sigcutx and float(z[2])>=10.**sigcuty) if z[0] in validIDs]) # False Positive\n",
    "    fn=len([[z[0],float(z[1]),float(z[2]),float(z[3]),float(z[4]),'FN'] for z in variable if (float(z[1])<10.**sigcutx or float(z[2])<10.**sigcuty) if z[0] in validIDs]) # False Negative\n",
    "    validErr = check_error(fp,fn,len(valid))\n",
    "    \n",
    "    #plotting_tools.plotLC(len(validErr), trainErr, validErr, path+'random', False, True, 'Trial number')\n",
    "    return trainErr, validErr\n",
    "        \n",
    "def check_error(fp,fn,total):\n",
    "    classif_err = float(fp+fn)/float(total)\n",
    "    return classif_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SKIP CELL COUNTING (tools.py)\n",
    "\n",
    "def SigmaFit(data):\n",
    "    median = np.median(data)\n",
    "    std_median = np.sqrt(np.mean([(i-median)**2. for i in data]))\n",
    "    tmp_data = [a for a in data if a < 3.*std_median+median and a > median - 3.*std_median]\n",
    "    param1 = norm.fit(tmp_data)\n",
    "    param2 = norm.fit(data)\n",
    "    return param1, param2\n",
    "\n",
    "def extract_data(filename):\n",
    "    # extract data in a csv file into a list\n",
    "    info=[]\n",
    "    data=open(filename,'r')\n",
    "    for lines in data:\n",
    "        if not lines.startswith(\"#\"):\n",
    "            lines=lines.rstrip().replace(\" \", \"\")\n",
    "            info.append(lines.split(','))\n",
    "    data.close()\n",
    "    return info\n",
    "\n",
    "def write_data(filename,tmp):\n",
    "    output = open(filename,'w')\n",
    "    for line in tmp:\n",
    "        output.write(str(line[0])+','+str(line[1])+','+str(line[2])+'\\n')\n",
    "    output.close()\n",
    "    return\n",
    "\n",
    "def read_src_lc(sources):\n",
    "#\n",
    "# Reads all the extracted source data and sorts them into unique sources with their lightcurves\n",
    "#\n",
    "    runcat=[] # A list comprising all the unique runcat source ids.\n",
    "    new_source={} # A dictionary containing all the lightcurves, etc, for each unique source\n",
    "    frequencies=[] # The frequencies in the dataset\n",
    "    for a in range(len(sources)):\n",
    "        new_runcat=sources[a][8]\n",
    "        new_freq=int((float(sources[a][7])/1e6)+0.5) # observing frequency in MHz\n",
    "        # check if it's a new runcat source and then either create a new entry or append\n",
    "        if new_runcat not in runcat:\n",
    "            runcat.append(new_runcat)\n",
    "            new_source[new_runcat]=[sources[a]]\n",
    "        else:\n",
    "            new_source[new_runcat]=new_source[new_runcat]+[sources[a]]\n",
    "        # If the observing frequency is new, append to the list\n",
    "        if new_freq not in frequencies: \n",
    "            frequencies.append(new_freq)\n",
    "\n",
    "    # return the list of observing frequencies and the full dictionary of source information\n",
    "    return frequencies, new_source\n",
    "\n",
    "def collate_trans_data(new_source,frequencies,transients):\n",
    "#\n",
    "# Using the data stored in the new_source dictionary and the transients list, store the transient and variability\n",
    "# parameters for each unique source in the dataset.\n",
    "#    \n",
    "    trans_data=[]\n",
    "    bands={}\n",
    "    # sort the information in the transients list into a dictionary\n",
    "    # transient id:[transient type, flux/max_rms, flux/min_rms, detection_thresh]\n",
    "    transRuncat={x[5]:[x[2],float(x[1])/float(x[3]), float(x[1])/float(x[4]), x[0]] for x in transients}\n",
    "    for freq in frequencies:\n",
    "        for keys in new_source.keys():\n",
    "            flux=[]\n",
    "            flux_err=[]\n",
    "            date=[]\n",
    "            band=[]\n",
    "            tmp=0.\n",
    "            # Extract the different parameters for the source\n",
    "            for b in range(len(new_source[keys])):\n",
    "                if int((float(new_source[keys][b][7])/1e6)+0.5)==freq:\n",
    "                    band.append(new_source[keys][b][0])\n",
    "                    flux.append(float(new_source[keys][b][5]))\n",
    "                    flux_err.append(float(new_source[keys][b][6]))\n",
    "                    if tmp<int(new_source[keys][b][14]):\n",
    "                        eta=float(new_source[keys][b][2])\n",
    "                        V=float(new_source[keys][b][11])\n",
    "                        N=float(new_source[keys][b][4])\n",
    "                        tmp=int(new_source[keys][b][14])\n",
    "                    ra=new_source[keys][b][-2]\n",
    "                    dec=new_source[keys][b][-3]\n",
    "            # if the source has been observed in the given observing frequency, extract the variability parameters\n",
    "            # from the final observation at that frequency.\n",
    "            if len(flux)!=0:\n",
    "                bands[freq]=band\n",
    "                ### Calculate the ratios...\n",
    "                avg_flux_ratio = [x/(sum(flux)/len(flux)) for x in flux]\n",
    "                ### Collate and store the transient parameters (these are across all the pipeline runs for the final figures)\n",
    "                if keys in transRuncat.keys():\n",
    "                # identify if this source is in the new source transient list and extract parameters\n",
    "                    transType=transRuncat[keys][0]\n",
    "                    min_sig=transRuncat[keys][1]\n",
    "                    max_sig=transRuncat[keys][2]\n",
    "                    detect_thresh=transRuncat[keys][3]\n",
    "                else:\n",
    "                # if not in the transient list, then insert standard parameters for non-transients\n",
    "                    transType=2\n",
    "                    min_sig=0\n",
    "                    max_sig=0\n",
    "                    detect_thresh=0\n",
    "                # write out the key parameters for each source at each observing frequency\n",
    "                trans_data.append([keys, eta, V, max(flux), max(avg_flux_ratio), freq, len(flux), ra, dec, transType, min_sig, max_sig, detect_thresh])\n",
    "    print 'Number of transients in sample: '+str(len(trans_data))\n",
    "    # Return the array of key parameters for each source\n",
    "    return trans_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SKIP CELL COUNTING (plotting)\n",
    "\n",
    "def make_cmap(frequencies):\n",
    "    cm = matplotlib.cm.get_cmap('hsv')\n",
    "    col = [cm(1.*i/len(frequencies)) for i in range(len(frequencies))]\n",
    "    return col\n",
    "\n",
    "def gaussian_fit(data,param):\n",
    "    range_data=np.linspace(min(data),max(data),1000)\n",
    "    fit=norm.pdf(range_data,loc=param[0],scale=param[1])\n",
    "    return range_data,fit\n",
    "\n",
    "def make_bins(x):\n",
    "    new_bins = density_estimation.bayesian_blocks(x)\n",
    "    binsx = [new_bins[a] for a in range(len(new_bins)-1) if abs((new_bins[a+1]-new_bins[a])/new_bins[a])>0.05]\n",
    "    binsx = binsx + [new_bins[-1]]\n",
    "    return binsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-a5ef82a5b27a>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-a5ef82a5b27a>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    print 'sigma_(eta_nu)='+str(best_sigma1)+', sigma_(V_nu)='+str(best_sigma2)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "###FIND PRECISION AND RECALL FOR GIVEN THRESHOLDS\n",
    "\n",
    "# Load the data and give appropriate labels\n",
    "all_data = generic_tools.load_data(stableData,simulatedData)\n",
    "\n",
    "# Load the simulations and only keep those with a full lightcurve and with variability parameters > 0\n",
    "all_data=all_data.loc[(all_data['ttype'] == 2) & (all_data['V']>0.) & (all_data['eta']>0.)]\n",
    "\n",
    "# put the training data into the format required for the training\n",
    "train_data = all_data.apply(lambda row:[np.log10(row['eta']),np.log10(row['V']),row['variable'],row['label']],axis=1)\n",
    "train_data = train_data.as_matrix()\n",
    "\n",
    "# Obtain the training data if it doesn't already exist on disk (SLOW!)\n",
    "if not os.path.exists(path+'sigma_data.txt'):\n",
    "    filename = open(path+\"sigma_data.txt\", \"w\")\n",
    "    filename.write('')\n",
    "    filename.close()\n",
    "    train_anomaly_detect.multiple_trials(train_data,path+\"sigma_data.txt\")\n",
    "data2=np.genfromtxt(path+'sigma_data.txt', delimiter=' ')\n",
    "\n",
    "# Using the training data, find the sigma threshold combination that best fits input thresholds\n",
    "best_sigma1, best_sigma2 = train_anomaly_detect.find_best_sigmas(precis_thresh,recall_thresh,data2,tests,train_data,plots,path)\n",
    "print 'sigma_(eta_nu)='+str(best_sigma1)+', sigma_(V_nu)='+str(best_sigma2)\n",
    "\n",
    "# Find the eta and V thresholds for the data\n",
    "sigcutx,paramx,range_x = generic_tools.get_sigcut([a[0] for a in train_data if a[2]==0.],best_sigma1)\n",
    "sigcuty,paramy,range_y = generic_tools.get_sigcut([a[1] for a in train_data if a[2]==0.],best_sigma2)\n",
    "print(r'Gaussian Fit $\\eta$: '+str(round(10.**paramx[0],2))+'(+'+str(round((10.**(paramx[0]+paramx[1])-10.**paramx[0]),2))+' '+str(round((10.**(paramx[0]-paramx[1])-10.**paramx[0]),2))+')')\n",
    "print(r'Gaussian Fit $V$: '+str(round(10.**paramy[0],2))+'(+'+str(round((10.**(paramy[0]+paramy[1])-10.**paramy[0]),2))+' '+str(round((10.**(paramy[0]-paramy[1])-10.**paramy[0]),2))+')')\n",
    "print 'Eta_nu threshold='+str(10.**sigcutx)+', V_nu threshold='+str(10.**sigcuty)\n",
    "threshx=10.**sigcutx\n",
    "threshy=10.**sigcuty\n",
    "\n",
    "# Calculate the false positives (FP), true negatives (TN), true positives (TP) and false negatives (FN)\n",
    "all_data.loc[(((all_data['eta']<threshx) | (all_data['V']<threshy)) & (all_data['variable'] == 1)),'classified'] = 'FN'\n",
    "all_data.loc[((all_data['eta']>=threshx) & (all_data['V']>=threshy) & (all_data['variable'] == 1)),'classified'] = 'TP'\n",
    "all_data.loc[(((all_data['eta']<threshx) | (all_data['V']<threshy)) & (all_data['variable'] == 0)),'classified'] = 'TN'\n",
    "all_data.loc[((all_data['eta']>=threshx) & (all_data['V']>=threshy) & (all_data['variable'] == 0)),'classified'] = 'FP'\n",
    "\n",
    "# Find candidates\n",
    "all_data.loc[(all_data['classified'] == 'FP')].to_csv(path+'AD_candidate_variables.csv',index=False)\n",
    "\n",
    "# Calculate the precision and recall\n",
    "precision, recall =  generic_tools.precision_and_recall(len(all_data.loc[(all_data['classified'] == 'TP')]),len(all_data.loc[(all_data['classified'] == 'FP')]),len(all_data.loc[(all_data['classified'] == 'FN')]))\n",
    "print \"Precision: \"+str(precision)+\", Recall: \"+str(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generic_tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-80830d455679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#load the data required\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneric_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstableData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msimulatedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generic_tools' is not defined"
     ]
    }
   ],
   "source": [
    "### TRAINING THE DATA\n",
    "\n",
    "\n",
    "# Inputs and settings\n",
    "precis_thresh = 0.99\n",
    "recall_thresh = 0.95\n",
    "lda=0.1\n",
    "detection_threshold=8\n",
    "path='ml_csv_files/'\n",
    "stableData = path+'stable_sources.csv'\n",
    "simulatedData = path+'sim_*_trans_data.csv'\n",
    "anomaly = True\n",
    "logistic = True\n",
    "transSrc = True\n",
    "# setting the options for the scipy optimise function\n",
    "options = {'full_output': True, 'maxiter': 5000, 'ftol': 1e-4, 'maxfun': 5000, 'disp': True}\n",
    "\n",
    "#load the data required\n",
    "all_data = generic_tools.load_data(stableData,simulatedData)\n",
    "\n",
    "#create the lambda curve for the logistic regreassion algorithm\n",
    "MLtests.lambda_curve(all_data,lda,options,path)\n",
    "\n",
    "# Create the learning curve for all three machine learning algorithms\n",
    "MLtests.learning_curve(anomaly,logistic,transSrc,all_data,lda,options,precis_thresh,recall_thresh,path,detection_threshold)\n",
    "\n",
    "# Create the repitition curve for all three machine learning algorithms\n",
    "MLtests.repeat_curve(anomaly,logistic,transSrc,all_data,lda,options,precis_thresh,recall_thresh,path,detection_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generic_tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a812b5982040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load the data and give appropriate labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeneric_tools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstableData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msimulatedData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m# Identify all the new sources detected during the pipeline run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mall_data2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ttype'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generic_tools' is not defined"
     ]
    }
   ],
   "source": [
    "# The input data and thresholds\n",
    "tests = False\n",
    "plots = False\n",
    "path='ml_csv_files/'\n",
    "stableData = path+'stable_sources.csv'\n",
    "simulatedData = path+'sim_*_trans_data.csv'\n",
    "detection_threshold = 8.\n",
    "\n",
    "# Load the data and give appropriate labels\n",
    "all_data = generic_tools.load_data(stableData,simulatedData)\n",
    "# Identify all the new sources detected during the pipeline run\n",
    "all_data2=all_data.loc[all_data['ttype'] != 2]\n",
    "# Create a histogram of the detection thresholds assuming the source\n",
    "# was found in the lowest RMS region of image\n",
    "x=all_data2.loc[all_data2['variable'] == 0]\n",
    "x2=all_data2.loc[all_data2['variable'] == 1]\n",
    "\n",
    "# Find the best margins to maximise detection and reliability\n",
    "best_data=all_data2[['minRmsSigma','variable']].as_matrix()\n",
    "worst_data=all_data2[['maxRmsSigma','variable']].as_matrix()\n",
    "best_plot_data, worst_plot_data, sigBest, sigWorst = train_sigma_margin.find_sigma_margin(best_data,worst_data, detection_threshold)\n",
    "\n",
    "# Search and identify the optimal sigma margin for the best and\n",
    "# worst parts of the image\n",
    "sigWorst, sigBest = train_sigma_margin.plot_diagnostic(best_plot_data,worst_plot_data,path)\n",
    "\n",
    "# Identify the ids of interesting transient candidates assuming they're in the\n",
    "# worst part of the image\n",
    "tmpData = all_data2.loc[(all_data2['maxRmsSigma'] >= sigWorst+detection_threshold) & (all_data2['variable']==0)]\n",
    "tmpData.to_csv(path+'candidate_transients_worst_region.csv',index=False)\n",
    "\n",
    "# Identify the ids of interesting transient candidates assuming they're in the\n",
    "# best part of the image\n",
    "tmpData = all_data2.loc[(all_data2['minRmsSigma'] >= sigBest+detection_threshold) & (all_data2['variable']==0)]\n",
    "tmpData.to_csv(path+'candidate_transients_best_region.csv',index=False)\n",
    "\n",
    "# And the best thresholds are:\n",
    "print('Lowest RMS region threshold: '+str(sigBest+detection_threshold)+' sigma')\n",
    "print('Highest RMS region threshold: '+str(sigWorst+detection_threshold)+' sigma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
